# Plan: Exploit token prefix property in mapping

## Summary

`_derive_slice_token` produces tokens that are prefixes of longer tokens
(the HMAC message is independent of `token_len`; the result is just
truncated base32).  The current mapping code ignores this property and
recomputes all HMACs from scratch at every candidate length during both
the minimum-uniqueness search and collision promotion.  Compute full-length
tokens once per file and truncate as needed.

## Problem

`_find_min_local_len` iterates `token_len` from 1 to `max_token_len`,
calling `_compute_tokens` at each step.  Each call invokes
`_derive_slice_token` for every slice, which computes
`base32(HMAC-SHA256(...))[:token_len]`.  The HMAC input does not include
`token_len`, so the underlying digest is identical every time -- only the
truncation length changes.  For a file with S slices and max token length
M, this performs `S * M` HMAC operations when `S` would suffice.

The collision-promotion loop has the same issue: promoting a file from
length N to N+1 recomputes all S HMACs when it could just re-truncate the
already-computed full-length tokens.

## Goal

- Compute `_derive_slice_token` exactly once per slice per file (at
  `max_token_len`), then derive shorter tokens by truncation.
- Eliminate `_compute_tokens` and `_find_min_local_len` as separate
  functions; the logic inlines cleanly into `apply_mapping`.
- No change to `_derive_slice_token` itself (it is extracted into the
  generated client and stager).
- Deterministic output is identical (same HMAC, same truncation, same
  canonical promotion order).

## Design

In `apply_mapping`, after computing `max_token_len` for a file:

1. Compute full-length tokens once:
   ```python
   full_tokens = tuple(
       _derive_slice_token(seed_bytes, pv, i, max_token_len)
       for i in range(total_slices)
   )
   ```

2. Find the minimum length that makes all tokens unique by checking
   prefixes:
   ```python
   local_len = None
   for length in range(1, max_token_len + 1):
       if len(set(t[:length] for t in full_tokens)) == total_slices:
           local_len = length
           break
   ```

3. Store `full_tokens` on the entry (as a transient key `_full_tokens`)
   so the collision-promotion loop can re-truncate without recomputing.

4. In the promotion loop, replace the `_compute_tokens` call with:
   ```python
   entry["slice_tokens"] = tuple(t[:new_len] for t in entry["_full_tokens"])
   ```

5. Before returning, strip `_full_tokens` from every entry so it does not
   leak into downstream data structures.

Delete `_compute_tokens` and `_find_min_local_len` -- their logic is fully
subsumed by the inlined code above.

## Affected Components

- `dnsdle/mapping.py`: delete `_compute_tokens` and `_find_min_local_len`;
  refactor `apply_mapping` to compute full-length tokens once and truncate.
  No other file imports these two functions, so no call-site updates needed
  elsewhere.

## Execution Notes

Executed 2026-02-21.  No deviations from the plan.

- Deleted `_compute_tokens` (5 lines) and `_find_min_local_len` (6 lines).
- In `apply_mapping`, full-length tokens are computed once per file via
  `_derive_slice_token(..., max_token_len)` and stored as `_full_tokens`.
- Minimum-uniqueness search checks `set(t[:length] ...)` inline.
- Collision promotion truncates from `_full_tokens` instead of recomputing.
- `_full_tokens` is deleted from every entry before returning.
- Validated: import succeeds, multi-file mapping produces collision-free
  output, prefix property confirmed independently.
- Commit: `dfc7b3a` (plan), `<see below>` (implementation).
